{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First, let’s learn about what exactly an environment is.**\n",
    "An environment contains all the necessary functionality to run an agent and allow it to learn.\n",
    "Each environment must implement the following gym interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne l'executez pas ce n'est que pour vous montrer la structure\n",
    "\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class CustomEnv(gym.Env):\n",
    "  \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "  metadata = {'render.modes': ['human']}\n",
    "\n",
    "  def __init__(self, arg1, arg2, ...):\n",
    "    super(CustomEnv, self).__init__()    # Define action and observation space\n",
    "    # They must be gym.spaces objects    # Example when using discrete actions:\n",
    "    self.action_space = spaces.Discrete(N_DISCRETE_ACTIONS)    # Example for using image as input:\n",
    "    self.observation_space = spaces.Box(low=0, high=255, shape=\n",
    "                    (HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
    "\n",
    "  def step(self, action):\n",
    "    # Execute one time step within the environment\n",
    "    ...  def reset(self):\n",
    "    # Reset the state of the environment to an initial state\n",
    "    ...  def render(self, mode='human', close=False):\n",
    "    # Render the environment to the screen\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the constructor, we first define the type and shape of our action_space, which will contain all of the actions possible for an agent to take in the environment. Similarly, we’ll define the observation_space, which contains all of the environment’s data to be observed by the agent.\n",
    "\n",
    "Our reset method will be called to periodically reset the environment to an initial state. This is followed by many steps through the environment, in which an action will be provided by the model and must be executed, and the next observation returned. This is also where rewards are calculated, more on this later.\n",
    "\n",
    "Finally, the render method may be called periodically to print a rendition of the environment. This could be as simple as a print statement, or as complicated as rendering a 3D environment using openGL. For this example, we will stick with print statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stock Trading Environment\n",
    "\n",
    "To demonstrate how this all works, we are going to create a stock trading environment. We will then train our agent to become a profitable trader within the environment. Let’s get started!\n",
    "\n",
    "The first thing we’ll need to consider is how a human trader would perceive their environment. What observations would they make before deciding to make a trade?\n",
    "\n",
    "A trader would most likely look at some charts of a stock’s price action, perhaps overlaid with a couple technical indicators. From there, they would combine this visual information with their prior knowledge of similar price action to make an informed decision of which direction the stock is likely to move.\n",
    "\n",
    "So let’s translate this into how our agent should perceive its environment.\n",
    "\n",
    "Our observation_space contains all of the input variables we want our agent to consider before making, or not making a trade. In this example, we want our agent to “see” the stock data points (open price, high, low, close, and daily volume) for the last five days, as well a couple other data points like its account balance, current stock positions, and current profit.\n",
    "\n",
    "The intuition here is that for each time step, we want our agent to consider the price action leading up to the current price, as well as their own portfolio’s status in order to make an informed decision for the next action.\n",
    "\n",
    "Once a trader has perceived their environment, they need to take an action. In our agent’s case, its action_space will consist of three possibilities: buy a stock, sell a stock, or do nothing.\n",
    "\n",
    "But this isn’t enough; we need to know the amount of a given stock to buy or sell each time. Using gym’s Box space, we can create an action space that has a discrete number of action types (buy, sell, and hold), as well as a continuous spectrum of amounts to buy/sell (0-100% of the account balance/position size respectively).\n",
    "\n",
    "You’ll notice the amount is not necessary for the hold action, but will be provided anyway. Our agent does not initially know this, but over time should learn that the amount is extraneous for this action.\n",
    "\n",
    "The last thing to consider before implementing our environment is the reward. We want to incentivize profit that is sustained over long periods of time. At each step, we will set the reward to the account balance multiplied by some fraction of the number of time steps so far.\n",
    "\n",
    "The purpose of this is to delay rewarding the agent too fast in the early stages and allow it to explore sufficiently before optimizing a single strategy too deeply. It will also reward agents that maintain a higher balance for longer, rather than those who rapidly gain money using unsustainable strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we’ve defined our observation space, action space, and rewards**, it’s time to implement our environment. First, we need define the action_space and observation_space in the environment’s constructor.\n",
    "The environment expects a pandas data frame to be passed in containing the stock data to be learned from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "class StockTradingEnvironment(gym.Env):\n",
    "  \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "\n",
    "  def __init__(self, df): # df est la dataframe des stocks actions\n",
    "        \n",
    "    metadata = {'render.modes': ['human']}  \n",
    "    super(StockTradingEnv, self).__init__()\n",
    "    self.df = df\n",
    "    self.reward_range = (0, MAX_ACCOUNT_BALANCE)     # Actions of the format Buy x%, Sell x%, Hold, etc.\n",
    "    self.action_space = spaces.Box(\n",
    "      low=np.array([0, 0]), high=np.array([3, 1]), dtype=np.float16)    # Prices contains the OHCL values for the last five prices\n",
    "    self.observation_space = spaces.Box(\n",
    "      low=0, high=1, shape=(6, 6), dtype=np.float16)\n",
    "    \n",
    "    \n",
    "  def reset(self):\n",
    "    # Reset the state of the environment to an initial state\n",
    "    self.balance = INITIAL_ACCOUNT_BALANCE\n",
    "    self.net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "    self.max_net_worth = INITIAL_ACCOUNT_BALANCE\n",
    "    self.shares_held = 0\n",
    "    self.cost_basis = 0\n",
    "    self.total_shares_sold = 0\n",
    "    self.total_sales_value = 0\n",
    "\n",
    "    # Set the current step to a random point within the data frame\n",
    "    self.current_step = random.randint(0, len(self.df.loc[:, 'Open'].values) - 6)  \n",
    "    return self._next_observation()\n",
    "\n",
    "  def step(self, action):\n",
    "  # Execute one time step within the environment\n",
    "    self._take_action(action)  \n",
    "    self.current_step += 1  \n",
    "    if self.current_step > len(self.df.loc[:, 'Open'].values) - 6:\n",
    "      self.current_step = 0  delay_modifier = (self.current_step / MAX_STEPS)\n",
    "    \n",
    "    reward = self.balance * delay_modifier\n",
    "    done = self.net_worth <= 0  \n",
    "    obs = self._next_observation()  \n",
    "    return obs, reward, done, {}\n",
    "    \n",
    "  def _next_observation(self):\n",
    "  # Get the data points for the last 5 days and scale to between 0-1\n",
    "    frame = np.array([\n",
    "                        self.df.loc[self.current_step: self.current_step +\n",
    "                                    5, 'Open'].values / MAX_SHARE_PRICE,\n",
    "                        self.df.loc[self.current_step: self.current_step +\n",
    "                                    5, 'High'].values / MAX_SHARE_PRICE,\n",
    "                        self.df.loc[self.current_step: self.current_step +\n",
    "                                    5, 'Low'].values / MAX_SHARE_PRICE,\n",
    "                        self.df.loc[self.current_step: self.current_step +\n",
    "                                    5, 'Close'].values / MAX_SHARE_PRICE,\n",
    "                        self.df.loc[self.current_step: self.current_step +\n",
    "                                    5, 'Volume'].values / MAX_NUM_SHARES,\n",
    "                       ])  # Append additional data and scale each value to between 0-1\n",
    "    obs = np.append(frame, [[\n",
    "            self.balance / MAX_ACCOUNT_BALANCE,\n",
    "            self.max_net_worth / MAX_ACCOUNT_BALANCE,\n",
    "            self.shares_held / MAX_NUM_SHARES,\n",
    "            self.cost_basis / MAX_SHARE_PRICE,\n",
    "            self.total_shares_sold / MAX_NUM_SHARES,\n",
    "            self.total_sales_value / (MAX_NUM_SHARES * MAX_SHARE_PRICE),\n",
    "          ]], axis=0)  \n",
    "    return obs\n",
    "\n",
    "  def _take_action(self, action):\n",
    "  # Set the current price to a random price within the time step\n",
    "    current_price = random.uniform(\n",
    "      self.df.loc[self.current_step, \"Open\"],\n",
    "      self.df.loc[self.current_step, \"Close\"])  action_type = action[0]\n",
    "    amount = action[1]  \n",
    "    if action_type < 1:\n",
    "    # Buy amount % of balance in shares\n",
    "      total_possible = self.balance / current_price\n",
    "      shares_bought = total_possible * amount\n",
    "      prev_cost = self.cost_basis * self.shares_held\n",
    "      additional_cost = shares_bought * current_price    self.balance -= additional_cost\n",
    "      self.cost_basis = (prev_cost + additional_cost) / \n",
    "                              (self.shares_held + shares_bought)\n",
    "      self.shares_held += shares_bought  \n",
    "    elif actionType < 2:\n",
    "    # Sell amount % of shares held\n",
    "      shares_sold = self.shares_held * amount . \n",
    "      self.balance += shares_sold * current_price\n",
    "      self.shares_held -= shares_sold\n",
    "      self.total_shares_sold += shares_sold\n",
    "      self.total_sales_value += shares_sold * current_price  \n",
    "      self.netWorth = self.balance + self.shares_held * current_price \n",
    "    \n",
    "    if self.net_worth > self.max_net_worth:\n",
    "      self.max_net_worth = net_worth\n",
    "    if self.shares_held == 0:\n",
    "      self.cost_basis = 0\n",
    "\n",
    "\n",
    "  def render(self, mode='human', close=False):\n",
    "  # Render the environment to the screen\n",
    "    profit = self.net_worth - INITIAL_ACCOUNT_BALANCE  \n",
    "    print(f'Step: {self.current_step}')\n",
    "    print(f'Balance: {self.balance}')\n",
    "    print(f'Shares held: {self.shares_held}(Total sold: {self.total_shares_sold})')\n",
    "    print(f'Avg cost for held shares: {self.cost_basis}(Total sales value: {self.total_sales_value})')\n",
    "    print(f'Net worth: {self.net_worth}(Max net worth: {self.max_net_worth})')\n",
    "    print(f'Profit: {profit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Now let's instanciate the Q learning process\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stable_baselines'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d059ddd17648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMlpPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPO2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2 \n",
    "from env.StockTradingEnv import StockTradingEnv\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./AAPL.csv')\n",
    "df = df.sort_values('Date')# The algorithms require a vectorized environment to run\n",
    "env = DummyVecEnv([lambda: StockTradingEnv(df)])\n",
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=20000)\n",
    "obs = env.reset()\n",
    "for i in range(2000):\n",
    "  action, _states = model.predict(obs)\n",
    "  obs, rewards, done, info = env.step(action)\n",
    "  env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}